# How Phase 1 Model Works

**Project:** OrchestAI - Phase 1 Supervised Pre-Training  
**Date:** December 2025  
**Status:** Phase 1 Training Complete

---

## Executive Summary

This document explains how the Phase 1 trained model works, from receiving a user prompt to executing a complete workflow. The Phase 1 model is a multi-component system that combines NLP understanding, task decomposition, graph generation, model selection, and orchestration to autonomously plan and execute complex multi-step tasks.

---

## Table of Contents

1. [Overview](#overview)
2. [System Architecture](#system-architecture)
3. [Step-by-Step Process](#step-by-step-process)
4. [Component Details](#component-details)
5. [Data Flow](#data-flow)
6. [Example Execution](#example-execution)
7. [What the Model Learned](#what-the-model-learned)
8. [Limitations and Next Steps](#limitations-and-next-steps)

---

## Overview

### What Phase 1 Model Does

The Phase 1 model is **not just an NLP model** - it's a **planning and orchestration system** that:

1. **Understands** user instructions (uses NLP)
2. **Decomposes** complex tasks into subtasks
3. **Generates** workflow graphs (DAGs)
4. **Selects** appropriate worker models
5. **Orchestrates** execution of the workflow

### Key Difference from Pure NLP

| Pure NLP Model | Phase 1 Model |
|----------------|---------------|
| Understands and responds | Understands, plans, and orchestrates |
| Single-step processing | Multi-step workflow execution |
| One model does everything | Routes to specialized models |
| No planning | Learned task planning |

---

## System Architecture

### High-Level Flow

```
User Prompt
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Instruction        â”‚  â† NLP Understanding (T5/BERT)
â”‚  Encoder            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Task               â”‚  â† Task Decomposition (LSTM)
â”‚  Decomposer         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Workflow Graph     â”‚  â† Graph Generation (GNN)
â”‚  Generator          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model Selector     â”‚  â† Worker Selection (Neural Network)
â”‚  (RL-based)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Orchestration      â”‚  â† Execution System
â”‚  System             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â†“
    Final Result
```

### Component Breakdown

1. **Instruction Encoder** (NLP)
   - Technology: T5/BERT
   - Purpose: Understand user prompt
   - Output: Dense vector embedding

2. **Task Decomposer** (LSTM)
   - Technology: LSTM neural network
   - Purpose: Break task into subtasks
   - Output: Sequence of subtasks with dependencies

3. **Workflow Graph Generator** (GNN)
   - Technology: Graph Neural Network (GCN/GAT/GraphSAGE)
   - Purpose: Create execution workflow (DAG)
   - Output: Directed acyclic graph

4. **Model Selector** (Neural Network)
   - Technology: Policy network (prepared for RL)
   - Purpose: Choose worker models for each subtask
   - Output: Worker assignments

5. **Orchestration System** (System Logic)
   - Technology: Python execution system
   - Purpose: Execute workflow
   - Output: Final results

---

## Step-by-Step Process

### Step 1: User Provides Prompt

**Input:**
```
"Summarize this document and create a presentation"
```

**What happens:**
- User sends natural language instruction
- System receives prompt as text string

---

### Step 2: NLP Understanding (Instruction Encoder)

**Component:** `InstructionEncoder` (T5/BERT-based)

**What it does:**
- Converts text to numerical representation
- Uses pre-trained language model (T5 or BERT)
- Captures semantic meaning, intent, task type

**Process:**
```python
instruction = "Summarize this document and create a presentation"
embedding = encoder(instruction)  # [768] or [512] dimensional vector
```

**Output:**
- Dense vector embedding representing the instruction
- Size: 768 (BERT) or 512 (T5) dimensions
- Contains semantic understanding of the task

**Why NLP is used:**
- Neural networks need numerical inputs
- Pre-trained models understand language better than random embeddings
- Captures meaning, not just words

---

### Step 3: Task Decomposition (Task Decomposer)

**Component:** `TaskDecomposer` (LSTM-based)

**What it does:**
- Breaks complex instruction into subtasks
- Predicts dependencies between subtasks
- Estimates task complexity
- Determines task types

**Process:**
```python
# LSTM sequentially generates subtasks
subtasks = decomposer(instruction_embedding)

# Output:
# Subtask 0: Extract text from document (dependencies: [])
# Subtask 1: Summarize text (dependencies: [0])
# Subtask 2: Create presentation outline (dependencies: [1])
# Subtask 3: Generate slides (dependencies: [2])
```

**How it works:**
1. LSTM processes instruction embedding
2. Sequentially generates subtask descriptions
3. Predicts which previous tasks each subtask depends on
4. Estimates complexity for each subtask
5. Determines when to stop generating

**Output:**
- Sequence of subtasks (typically 2-20 subtasks)
- Dependencies for each subtask
- Task types (text_extraction, summarization, generation, etc.)
- Complexity estimates

**What it learned:**
- From 170 training examples
- Patterns of how to break down tasks
- Common dependency structures
- Typical task sequences

---

### Step 4: Workflow Graph Generation (GNN)

**Component:** `WorkflowGraphGenerator` (Graph Neural Network)

**What it does:**
- Creates a directed acyclic graph (DAG) from subtasks
- Represents execution order and dependencies
- Ensures valid workflow structure (no cycles)

**Process:**
```python
# GNN processes subtask embeddings
graph = graph_generator(subtask_embeddings, dependencies)

# Creates graph structure:
#    0 â†’ 1 â†’ 2 â†’ 3
#    (Extract â†’ Summarize â†’ Outline â†’ Generate)
```

**How it works:**
1. Takes subtask embeddings from decomposer
2. Uses Graph Neural Network (GCN/GAT/GraphSAGE) to process
3. Learns relationships between subtasks
4. Generates node embeddings (representations of each task)
5. Predicts edges (dependencies) between nodes
6. Ensures DAG structure (no cycles)

**GNN Types Supported:**
- **GCN (Graph Convolutional Network):** Standard graph convolution
- **GAT (Graph Attention Network):** Attention-based aggregation
- **GraphSAGE:** Sampling and aggregation approach

**Output:**
- Directed acyclic graph (DAG)
- Node embeddings (learned representations of tasks)
- Edge probabilities (likelihood of dependencies)
- Adjacency matrix (binary graph structure)

**Why GNN:**
- Workflows are naturally graph-structured
- GNNs can learn complex dependency patterns
- Ensures valid DAG structure (no cycles)
- Better than sequential models for complex workflows

**DAG Validity:**
- âœ… 100% valid DAGs (no cycles)
- âœ… Proper topological ordering
- âœ… Dependencies respected

---

### Step 5: Model Selection (RL Model Selector)

**Component:** `RLModelSelector` (Policy Network)

**What it does:**
- Selects optimal worker model for each subtask
- Considers task characteristics and model capabilities
- Learned from training data (170 examples)

**Process:**
```python
# For each subtask, select a worker
for subtask in subtasks:
    state = combine(instruction_embedding, node_embedding)
    worker_id = model_selector.select_worker(state)
    
# Example selections:
# Subtask 0 â†’ Worker 4 (document processor)
# Subtask 1 â†’ Worker 0 (GPT-4 for quality)
# Subtask 2 â†’ Worker 1 (GPT-3.5 for cost efficiency)
```

**How it works:**
1. Combines instruction embedding with node embedding
2. Creates state representation for each subtask
3. Policy network outputs probability distribution over workers
4. Selects worker with highest probability (or samples)
5. Learned from training data patterns

**What it learned:**
- Which workers are good for which task types
- Cost-quality trade-offs
- Patterns from 170 training examples
- âš ï¸ Not yet optimized for outcomes (needs Phase 2 RL)

**Output:**
- Worker assignment for each subtask
- Action probabilities (confidence in selection)
- Value estimates (expected performance)

**Available Workers:**
- GPT-4 (high quality, expensive)
- GPT-3.5-turbo (balanced)
- LLaMA-3-8b (cost-effective)
- CLIP (vision)
- Whisper (audio)
- Code models, document processors, etc.

---

### Step 6: Workflow Execution (Orchestration System)

**Component:** `OrchestrationSystem`

**What it does:**
- Executes workflow following topological order
- Manages dependencies between tasks
- Handles parallel execution
- Combines results

**Process:**
```python
# Topological sort determines execution order
execution_order = topological_sort(workflow_graph)

# Execute tasks in order
for task in execution_order:
    # Wait for dependencies
    wait_for_dependencies(task)
    
    # Execute task with selected worker
    result = worker_layer.execute_task(
        worker_id=selected_worker,
        task=task_description,
        data=task_input
    )
    
    # Store result for dependent tasks
    task_results[task.id] = result

# Combine final results
final_output = combine_outputs(task_results)
```

**How it works:**
1. **Topological Sort:** Determines execution order respecting dependencies
2. **Dependency Resolution:** Ensures tasks only execute when dependencies are ready
3. **Parallel Execution:** Executes independent tasks concurrently (up to max_parallel_tasks)
4. **Worker Execution:** Routes each task to selected worker model
5. **Result Passing:** Passes outputs from one task to dependent tasks
6. **Output Combination:** Combines all results into final output

**Features:**
- âœ… Parallel execution of independent tasks
- âœ… Automatic retry on failures
- âœ… Cost and latency tracking
- âœ… Error handling
- âœ… Execution logging

**Output:**
- Final combined result
- Success/failure status
- Total cost
- Total latency
- Per-task metrics

---

## Data Flow

### Complete Data Flow

```
User Prompt (text)
    â†“
Instruction Encoder
    â†’ [768-dim embedding]
    â†“
Task Decomposer (LSTM)
    â†’ [subtask_embeddings: [batch, num_subtasks, 512]]
    â†’ [dependencies: [batch, num_subtasks, max_subtasks]]
    â†’ [task_types: [batch, num_subtasks, 10]]
    â†“
Workflow Graph Generator (GNN)
    â†’ [node_embeddings: [batch, num_subtasks, 128]]
    â†’ [adjacency: [batch, num_subtasks, num_subtasks]]
    â†’ [DAG structure]
    â†“
Model Selector (Policy Network)
    â†’ [worker_selections: [num_subtasks]]
    â†’ [action_probs: [num_subtasks, num_workers]]
    â†“
Orchestration System
    â†’ [execution_order: List[int]]
    â†’ [task_results: Dict[int, WorkerOutput]]
    â†“
Final Result
    â†’ [outputs: Dict]
    â†’ [success: bool]
    â†’ [cost: float]
    â†’ [latency: float]
```

### Tensor Shape Progression

```
Text: "Summarize document..."
    â†“
[1, 768]  â† Instruction embedding
    â†“
[1, 20, 512]  â† Subtask embeddings (20 subtasks)
    â†“
[1, 20, 128]  â† Node embeddings
    â†“
[1, 20, 20]  â† Adjacency matrix (DAG)
    â†“
[20]  â† Worker selections (one per subtask)
    â†“
Execution Results
```

---

## Example Execution

### Example 1: Simple Task

**User Prompt:**
```
"Summarize this text: Machine learning is a subset of AI."
```

**Step-by-Step:**

1. **NLP Understanding:**
   - Encoder understands: "This is a summarization task"
   - Creates embedding: [768-dim vector]

2. **Task Decomposition:**
   - Creates 1-2 subtasks:
     - Subtask 0: Summarize text (dependencies: [])

3. **Graph Generation:**
   - Creates simple graph: Single node (no dependencies)

4. **Model Selection:**
   - Selects: GPT-4 or GPT-3.5 for summarization

5. **Execution:**
   - Executes summarization
   - Returns: Summary text

**Result:**
- Success: True
- Output: "Machine learning is a subset of artificial intelligence."
- Cost: $0.002
- Latency: 500ms

---

### Example 2: Complex Task

**User Prompt:**
```
"Summarize this document and create a presentation"
```

**Step-by-Step:**

1. **NLP Understanding:**
   - Encoder understands: "Multi-step task: summarization + generation"
   - Creates embedding: [768-dim vector]

2. **Task Decomposition:**
   - Creates 4-5 subtasks:
     - Subtask 0: Extract text (dependencies: [])
     - Subtask 1: Summarize text (dependencies: [0])
     - Subtask 2: Create outline (dependencies: [1])
     - Subtask 3: Generate slides (dependencies: [2])

3. **Graph Generation:**
   - Creates DAG:
     ```
     0 â†’ 1 â†’ 2 â†’ 3
     ```
   - Valid DAG (no cycles)

4. **Model Selection:**
   - Subtask 0 â†’ Document processor
   - Subtask 1 â†’ GPT-4 (quality)
   - Subtask 2 â†’ GPT-3.5 (cost-effective)
   - Subtask 3 â†’ GPT-3.5 (generation)

5. **Execution:**
   - Executes in order: 0 â†’ 1 â†’ 2 â†’ 3
   - Passes outputs between tasks
   - Combines final result

**Result:**
- Success: True
- Output: Presentation content
- Cost: $0.045
- Latency: 2000ms
- Tasks: 4 subtasks executed

---

## What the Model Learned

### From Training Data (170 Examples)

**Task Decomposition Patterns:**
- How to break down common task types
- Typical number of subtasks for different tasks
- Common dependency patterns
- Task sequencing patterns

**Workflow Structures:**
- Valid DAG structures
- Dependency relationships
- Task ordering patterns
- Graph connectivity patterns

**Model Selection Patterns:**
- Which workers are good for which tasks
- Cost-quality trade-offs
- Worker capabilities
- Selection heuristics

**Limitations:**
- Learned from 170 examples (limited)
- May not generalize to all task types
- Selection not optimized for outcomes (needs Phase 2 RL)
- Moderate accuracy (loss ~2.0)

---

## Component Details

### 1. Instruction Encoder

**Technology:** T5-base or BERT-base-uncased

**Purpose:**
- Convert natural language to dense vectors
- Capture semantic meaning
- Provide fixed-size embeddings

**Configuration:**
- Model: T5-base (512 hidden) or BERT (768 hidden)
- Max length: 512 tokens
- Output: Fixed-size embedding

**Why T5/BERT:**
- Pre-trained on large text corpora
- Understands language semantics
- Better than training from scratch
- Transfer learning benefits

---

### 2. Task Decomposer

**Technology:** LSTM (Long Short-Term Memory)

**Purpose:**
- Sequentially generate subtasks
- Predict dependencies
- Estimate complexity

**Architecture:**
- Encoder: Linear layers + LayerNorm
- LSTM: 3 layers, 512 hidden size
- Decoders:
  - Description decoder
  - Task type classifier
  - Dependency predictor
  - Complexity estimator
  - Stop predictor

**Why LSTM:**
- Sequential generation (one subtask at a time)
- Can model dependencies on previous subtasks
- Variable-length output (different number of subtasks)
- Proven for sequence generation

**Output:**
- Subtask embeddings: [batch, num_subtasks, 512]
- Task types: [batch, num_subtasks, 10]
- Dependencies: [batch, num_subtasks, max_subtasks]
- Complexities: [batch, num_subtasks, 1]

---

### 3. Workflow Graph Generator

**Technology:** Graph Neural Network (GCN/GAT/GraphSAGE)

**Purpose:**
- Generate workflow graphs (DAGs)
- Learn task relationships
- Ensure valid structure

**Architecture:**
- Input projection: Linear layer
- GNN layers: 3 layers (GCN/GAT/GraphSAGE)
- Output projection: Linear layers
- Edge predictor: Binary classifier

**GNN Types:**
- **GCN:** Standard graph convolution
- **GAT:** Attention-based (4 heads)
- **GraphSAGE:** Sampling-based

**Why GNN:**
- Workflows are graph-structured
- Can learn complex dependency patterns
- Natural fit for DAG generation
- Better than sequential models

**DAG Validation:**
- Ensures no cycles
- Respects topological ordering
- Validates graph structure
- 100% valid DAGs generated

**Output:**
- Node embeddings: [batch, num_subtasks, 128]
- Edge probabilities: [batch, num_subtasks, num_subtasks]
- Adjacency matrix: [batch, num_subtasks, num_subtasks] (binary)

---

### 4. Model Selector

**Technology:** Policy Network (prepared for RL)

**Purpose:**
- Select optimal workers for subtasks
- Route tasks to appropriate models
- Learn selection patterns

**Architecture:**
- Policy network: MLP (256, 128 â†’ num_workers)
- Value network: MLP (256, 128 â†’ 1)
- State: [instruction_embedding, node_embedding]

**Current State (Phase 1):**
- Learned from training data
- Pattern-based selection
- Not optimized for outcomes yet

**Future (Phase 2):**
- RL fine-tuning
- Optimize for success rate
- Optimize for cost/latency
- Learn from execution feedback

**Output:**
- Worker selections: [num_subtasks]
- Action probabilities: [num_subtasks, num_workers]
- Value estimates: [num_subtasks, 1]

---

### 5. Orchestration System

**Technology:** Python execution system

**Purpose:**
- Execute workflow
- Manage dependencies
- Handle parallel execution
- Combine results

**Features:**
- Topological sorting
- Dependency resolution
- Parallel execution (up to 5 concurrent tasks)
- Retry mechanisms
- Error handling
- Cost/latency tracking
- Execution logging

**Execution Flow:**
1. Topological sort â†’ execution order
2. Track ready tasks (dependencies satisfied)
3. Execute ready tasks in parallel
4. Pass results to dependent tasks
5. Combine final outputs

---

## Limitations and Next Steps

### Current Limitations (Phase 1)

1. **Accuracy:**
   - Loss ~2.0 (higher than ideal <0.5)
   - Moderate task decomposition accuracy
   - Model selection not optimal

2. **Optimization:**
   - Not optimized for cost/latency
   - Not optimized for success rate
   - Pattern-based, not outcome-based

3. **Data:**
   - Trained on 170 examples (limited)
   - May not generalize to all task types
   - More data would improve accuracy

4. **Generalization:**
   - May struggle with unseen task types
   - Limited to training data distribution
   - Needs more diverse examples

### Next Steps (Phase 2)

1. **RL Fine-Tuning:**
   - Optimize for success rate
   - Optimize for cost efficiency
   - Optimize for latency
   - Learn from execution feedback

2. **More Data:**
   - Collect 100+ more execution logs
   - Improve generalization
   - Better accuracy

3. **Evaluation:**
   - Test on diverse tasks
   - Measure performance metrics
   - Compare with baselines

---

## Summary

### How Phase 1 Works

1. **User Prompt** â†’ NLP understands (T5/BERT)
2. **Understanding** â†’ Task decomposer breaks into subtasks (LSTM)
3. **Subtasks** â†’ GNN creates workflow graph (GCN/GAT/GraphSAGE)
4. **Graph** â†’ Model selector chooses workers (Policy Network)
5. **Workers** â†’ Orchestrator executes workflow (System Logic)
6. **Execution** â†’ Final result returned

### Key Technologies

- **NLP:** T5/BERT for understanding
- **LSTM:** Sequential task decomposition
- **GNN:** Graph-based workflow generation
- **Neural Networks:** Model selection
- **System Logic:** Workflow execution

### What Makes It Special

- **Learned Planning:** Not rule-based, learns from data
- **Graph Structure:** Natural representation of workflows
- **Multi-Model:** Routes to specialized workers
- **Autonomous:** No manual configuration needed
- **Scalable:** Can handle complex multi-step tasks

### Current Status

âœ… **Working:** Model successfully decomposes tasks, generates workflows, selects workers, and executes  
âš ï¸ **Needs Improvement:** Accuracy can be better, optimization needed  
ğŸš€ **Next:** Phase 2 RL fine-tuning for optimization

---

**The Phase 1 model is a complete planning and orchestration system that learned to autonomously handle complex multi-step tasks.**

---

**Document Prepared By:** OrchestAI Development Team  
**Date:** December 2025  
**Status:** Phase 1 Complete, Documentation Complete


